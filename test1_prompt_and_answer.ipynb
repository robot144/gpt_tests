{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt and answer\n",
    "\n",
    "This notebook shows how to use the API of chat-gpt in python. It contains a simple prompt and answer script. In its simplest form you send a question (called a prompt) to API at openai.com and the reply of the GPT model is returned. \n",
    "There are a few general properties to keep in mind:\n",
    "- The pre-learning was performed by letting the model predict the next word. This model was trained on an enormous amount of text, mainly scraped from internet.\n",
    "- It is a language model, and it performs well in many languages, but excells in English. This is probably caused by the larger amount of training material in English. \n",
    "- It sees programming languages simply as langauges. It knows of many programming languages, but is probably best in python. Again probably caused by the amount of training material.\n",
    "- The model is generative, which means that it generates text with a random touch to it. Twice the same question will generally give two different answers.\n",
    "- The style is kept separate from the content, so you can give style hints, like: \"describe to in laymen's terms what generative deep learning models are.\" You can look at https://prompts.chat for inspiration.\n",
    "- You will need an api-key to get access and for billing to an account.\n",
    "- Every api call will cost money. At the moment roughly 0.3 cents per thousand tokens for GPT-3.5-turbo, and a token is roughly a syllable long. GPT-4 is more capable but 10 times as expensive, and not everyone get's access.\n",
    "\n",
    "There are many things you can ask. Here are a few examples:\n",
    "- Can you explain the prandtl length scale model for turbulence\n",
    "- Can you give an example of how to make a gif movie with matplotlib in python\n",
    "- You are a translation robot from english to french. You will receive a text in english and will reply in french. Just give the answer. \"To be or not to be, that's the question.\"\n",
    "\n",
    "More info can be found at:\n",
    "- https://www.datacamp.com/tutorial/using-gpt-models-via-the-openai-api-in-python\n",
    "- https://platform.openai.com/docs/quickstart\n",
    "- https://platform.openai.com/docs/api-reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installation on colab\n",
    "# run this cell to install the required packages, but only once\n",
    "# You may have to restart the runtime after running this cell (Runtime -> Restart runtime)\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "  %pip install openai\n",
    "  # getpass already installed\n",
    "else:\n",
    "  print('Make sure that you have installed the required packages listed in requirements.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import openai\n",
    "import getpass\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input your API key\n",
    "openai.api_key = getpass.getpass(\"Enter your API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the API to generate the answer\n",
    "max_tokens = 64\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\", # this is a simpler model than gpt-3, but it's still often useful\n",
    "    prompt=\"Translate the following English text to French: 'I love it when a plan comes together'\",\n",
    "    max_tokens=max_tokens\n",
    ")\n",
    "\n",
    "print(response.choices[0].text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show complete response\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The gtp3 and gpt4 models use a chat api\n",
    "\n",
    "The dialog between you and gpt takes the form of a list of messages, where each adds an item to the list in turn. The three most important roles are:\n",
    "- system: here you tell gpt what role to play with the input that follows, e.g. \"you are weird scientist giving evasive answers\"\n",
    "- user: the input into the model\n",
    "- assistant: the replies from gpt. If you input these for the next question, then you will provide the history of the Q&A and can also refer back to previous questions.\n",
    "\n",
    "The inputs and outputs of the model are structures with fields and lists. In the web-api they're given in JSON format, but in python they're translated into python structures like:\n",
    "\n",
    "``[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the available engines, which will depend on your API key\n",
    "response = openai.Model.list()\n",
    "\n",
    "#print(response[\"data\"][1])\n",
    "for item in response['data']:\n",
    "    id = item['id']\n",
    "    if \"gpt\" in id:\n",
    "        print(\"- \",id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some settings\n",
    "#model: typically use \"text-davinci-003\" for testing, \"gpt-3.5-turbo\" for production and \"gpt-4\" for difficult questions\n",
    "model = \"gpt-3.5-turbo\" \n",
    "max_tokens = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3.5 Turbo \n",
    "# The input to this model slightly more complex than the previous model. It looks like this:\n",
    "# [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    "response = openai.ChatCompletion.create( #note that we use ChatCompletion instead of Completion\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    max_tokens=max_tokens\n",
    ")\n",
    "\n",
    "print(\"Full response:\",response)\n",
    "print(\"=====================================\")\n",
    "print(\"Just the actual reply:\",response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple loop to keep asking for prompts\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}] # initialize with a system message\n",
    "display(Markdown(\"Type quit to stop \")) \n",
    "while True:\n",
    "    prompt = input(\"Enter your prompt: \")\n",
    "    if prompt == \"quit\":\n",
    "        break\n",
    "    #print(\"You: \" + prompt)\n",
    "    display(Markdown(\"__You:__ \" + prompt))\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt}) # add the user message to the list of messages\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    answer = response[\"choices\"][0][\"message\"]\n",
    "    #add the respose to the list of messages, so that it can be used as input for the next prompt\n",
    "    messages.append({\"role\": \"system\", \"content\": answer[\"content\"]}) \n",
    "    #print(\"GPT:\",answer[\"content\"])\n",
    "    display(Markdown(\"__GPT:__ \" + answer[\"content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygeneral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
